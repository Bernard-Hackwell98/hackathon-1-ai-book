# Glossary of Physical AI Terms

## A

**Artificial Intelligence (AI)**: The simulation of human intelligence processes by computer systems, especially machine learning and deep learning algorithms.

**Autonomous Vehicle**: A vehicle capable of sensing its environment and operating without human involvement, typically using perception-action systems.

## C

**Camera Calibration**: The process of determining the intrinsic and extrinsic parameters of a camera to correct for distortions and establish accurate measurements.

**Closed-loop Control**: A control system that uses feedback to continuously adjust its behavior based on the difference between desired and actual output.

**Computer Vision**: A field of artificial intelligence that trains computers to interpret and understand the visual world, using digital images/video and machine learning.

**Control Loop Frequency**: The rate at which a control system updates its actions based on feedback, typically measured in Hertz (Hz).

## D

**Deep Learning**: A subset of machine learning that uses artificial neural networks with multiple layers to model and understand complex patterns.

**Depth Perception**: The ability to perceive the world in three dimensions and judge the distance of objects, critical in stereo vision systems.

**Direct Method (SLAM)**: A category of visual SLAM approaches that use pixel intensities directly rather than extracting features.

## E

**Embodied Cognition**: The theory that cognitive processes are influenced by the body's interactions with the environment.

**Embodiment**: The concept that intelligence emerges from the interaction between an agent and its physical environment.

**Encoder**: A device that measures the rotation of a motor or wheel, providing relative position information for robot navigation.

## F

**Feature Detection**: The technique of identifying distinctive points in an image that can be used for matching, tracking, or recognition.

**Feature Matching**: The process of finding corresponding features between different images or frames.

**Field of View (FOV)**: The extent of the observable world that a camera can see at any given moment.

## H

**Hybrid Approach (Visual Servoing)**: A method that combines both position-based and image-based visual servoing techniques.

## I

**Image-Based Visual Servoing (IBVS)**: A visual servoing approach that directly uses image features to compute control actions without explicitly estimating 3D pose.

**Image Transport**: A ROS package that provides transparent support for transporting images in various compressed formats.

**Inertial Measurement Unit (IMU)**: A device that measures and reports a body's specific force, angular rate, and sometimes magnetic field, used for navigation and motion tracking.

## K

**Kalman Filter**: A mathematical method that uses a series of measurements observed over time to estimate unknown variables, often used in sensor fusion.

## L

**Latency**: The time delay between a stimulus and the resulting response, critical in real-time perception-action systems.

**Loop Closure**: In SLAM, the process of recognizing when a robot returns to a previously visited location to correct accumulated drift.

## M

**Machine Learning**: A type of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed.

**Multi-sensor Fusion**: The process of combining data from multiple sensors to improve perception accuracy and robustness.

## O

**Object Detection**: The computer vision task of identifying and locating objects within an image or video.

**OpenCV**: An open-source computer vision and machine learning software library.

**Optical Flow**: The pattern of apparent motion of objects, surfaces, and edges in a visual scene caused by the relative motion between an observer and the scene.

## P

**Perception-Action Loop**: The continuous cycle of sensing the environment, processing information, taking action, and receiving feedback.

**Position-Based Visual Servoing (PBVS)**: A visual servoing approach that estimates the 3D pose of the target object and computes the required camera motion.

**Point Cloud**: A set of data points in space, representing the external surface of an object or environment, typically generated by 3D scanners or depth cameras.

**Pose Estimation**: The process of determining the position and orientation of an object or camera in 3D space.

## R

**Real-time Processing**: Processing data as it becomes available, with timing constraints that require immediate response.

**RGB-D Camera**: A camera that captures both color (RGB) and depth (D) information simultaneously.

**Robotic Operating System (ROS)**: Flexible framework for writing robot software, providing services like hardware abstraction, device drivers, and message passing.

**ROS 2**: The second generation of the Robot Operating System, designed for production environments with improved security and real-time capabilities.

## S

**Sensor Fusion**: The process of combining data from multiple sensors to achieve improved accuracy and reliability.

**Simultaneous Localization and Mapping (SLAM)**: The computational problem of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location within it.

**Stereo Vision**: A technique that extracts depth information from 2D images using two or more cameras positioned at different angles.

**Structure from Motion (SfM)**: A photogrammetric range imaging technique for estimating 3D structures from 2D image sequences.

## V

**Visual Servoing**: A control strategy that uses visual feedback to control the motion of a robot.

**Vision Pipeline**: The sequence of processing steps applied to visual data, from acquisition to action planning.

**Vision Processing**: The application of algorithms to extract meaningful information from visual data.

## W

**Wheel Encoder**: A sensor that measures the rotation of a robot's wheels to estimate distance traveled and assist in localization.