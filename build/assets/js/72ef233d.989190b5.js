"use strict";(globalThis.webpackChunkai_book_ros2_module=globalThis.webpackChunkai_book_ros2_module||[]).push([[457],{5680(e,t,n){n.d(t,{xA:()=>c,yg:()=>y});var i=n(6540);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter(function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable})),n.push.apply(n,i)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach(function(t){a(e,t,n[t])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach(function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))})}return e}function s(e,t){if(null==e)return{};var n,i,a=function(e,t){if(null==e)return{};var n,i,a={},r=Object.keys(e);for(i=0;i<r.length;i++)n=r[i],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(i=0;i<r.length;i++)n=r[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=i.createContext({}),p=function(e){var t=i.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=p(e.components);return i.createElement(l.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},u=i.forwardRef(function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=p(n),y=a,g=u["".concat(l,".").concat(y)]||u[y]||m[y]||r;return n?i.createElement(g,o(o({ref:t},c),{},{components:n})):i.createElement(g,o({ref:t},c))});function y(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,o=new Array(r);o[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:a,o[1]=s;for(var p=2;p<r;p++)o[p]=n[p];return i.createElement.apply(null,o)}return i.createElement.apply(null,n)}u.displayName="MDXCreateElement"},7002(e,t,n){n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>p});var i=n(8168),a=(n(6540),n(5680));const r={},o="Base Models/Entities for Physical AI Module",s={unversionedId:"modules/physical-ai/entities",id:"modules/physical-ai/entities",title:"Base Models/Entities for Physical AI Module",description:"Physical AI",source:"@site/docs/modules/physical-ai/entities.md",sourceDirName:"modules/physical-ai",slug:"/modules/physical-ai/entities",permalink:"/hackathon-1-ai-book/modules/physical-ai/entities",draft:!1,editUrl:"https://github.com/bernard-hackwell98/hackathon-1-ai-book/tree/main/docs/modules/physical-ai/entities.md",tags:[],version:"current",frontMatter:{}},l={},p=[{value:"Physical AI",id:"physical-ai",level:2},{value:"Vision Primitives",id:"vision-primitives",level:2},{value:"Perception-Action Systems",id:"perception-action-systems",level:2}],c={toc:p};function m({components:e,...t}){return(0,a.yg)("wrapper",(0,i.A)({},c,t,{components:e,mdxType:"MDXLayout"}),(0,a.yg)("h1",{id:"base-modelsentities-for-physical-ai-module"},"Base Models/Entities for Physical AI Module"),(0,a.yg)("h2",{id:"physical-ai"},"Physical AI"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Name"),": Physical AI"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Description"),": Integration of artificial intelligence with physical systems, emphasizing perception-action loops"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Fields/Components"),":",(0,a.yg)("ul",{parentName:"li"},(0,a.yg)("li",{parentName:"ul"},"Perception: Sensing and understanding the environment"),(0,a.yg)("li",{parentName:"ul"},"Action: Physical responses to environmental stimuli"),(0,a.yg)("li",{parentName:"ul"},"Learning: Adaptation based on perception-action outcomes"),(0,a.yg)("li",{parentName:"ul"},"Embodiment: The physical form that enables interaction with the world"))),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Relationships"),": Connects computer vision, robotics, and AI systems")),(0,a.yg)("h2",{id:"vision-primitives"},"Vision Primitives"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Name"),": Vision Primitives"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Description"),": Core computer vision components in ROS 2 including image transport, camera interfaces, and point cloud processing"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Fields/Components"),":",(0,a.yg)("ul",{parentName:"li"},(0,a.yg)("li",{parentName:"ul"},"Image Transport: Mechanisms for passing image data between nodes"),(0,a.yg)("li",{parentName:"ul"},"Camera Interfaces: Standardized ways to connect cameras to ROS 2"),(0,a.yg)("li",{parentName:"ul"},"Point Cloud Processing: 3D data processing from depth sensors"),(0,a.yg)("li",{parentName:"ul"},"Stereo Vision: Depth perception using multiple cameras"),(0,a.yg)("li",{parentName:"ul"},"Object Detection: Identifying objects in visual data"))),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Relationships"),": These primitives are implemented using the underlying ROS 2 Architecture")),(0,a.yg)("h2",{id:"perception-action-systems"},"Perception-Action Systems"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Name"),": Perception-Action Systems"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Description"),": Closed-loop systems that integrate visual perception with robotic action"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Fields/Components"),":",(0,a.yg)("ul",{parentName:"li"},(0,a.yg)("li",{parentName:"ul"},"Closed-loop Control: Feedback systems using visual input"),(0,a.yg)("li",{parentName:"ul"},"Visual Servoing: Controlling robot motion based on visual feedback"),(0,a.yg)("li",{parentName:"ul"},"SLAM Integration: Simultaneous localization and mapping with vision"),(0,a.yg)("li",{parentName:"ul"},"Multi-sensor Fusion: Combining vision with other sensors (IMU, LIDAR)"),(0,a.yg)("li",{parentName:"ul"},"Real-world Applications: Practical implementations of perception-action systems"))),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Relationships"),": Connects vision processing with robotic control systems")))}m.isMDXComponent=!0}}]);