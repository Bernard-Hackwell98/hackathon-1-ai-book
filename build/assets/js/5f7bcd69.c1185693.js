"use strict";(globalThis.webpackChunkai_book_ros2_module=globalThis.webpackChunkai_book_ros2_module||[]).push([[11],{4353(e,t,n){n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>p});var a=n(8168),r=(n(6540),n(5680));const o={},i="Glossary of Physical AI Terms",s={unversionedId:"modules/physical-ai/glossary",id:"modules/physical-ai/glossary",title:"Glossary of Physical AI Terms",description:"A",source:"@site/docs/modules/physical-ai/glossary.md",sourceDirName:"modules/physical-ai",slug:"/modules/physical-ai/glossary",permalink:"/hackathon-1-ai-book/modules/physical-ai/glossary",draft:!1,editUrl:"https://github.com/bernard-hackwell98/hackathon-1-ai-book/tree/main/docs/modules/physical-ai/glossary.md",tags:[],version:"current",frontMatter:{}},l={},p=[{value:"A",id:"a",level:2},{value:"C",id:"c",level:2},{value:"D",id:"d",level:2},{value:"E",id:"e",level:2},{value:"F",id:"f",level:2},{value:"H",id:"h",level:2},{value:"I",id:"i",level:2},{value:"K",id:"k",level:2},{value:"L",id:"l",level:2},{value:"M",id:"m",level:2},{value:"O",id:"o",level:2},{value:"P",id:"p",level:2},{value:"R",id:"r",level:2},{value:"S",id:"s",level:2},{value:"V",id:"v",level:2},{value:"W",id:"w",level:2}],c={toc:p};function g({components:e,...t}){return(0,r.yg)("wrapper",(0,a.A)({},c,t,{components:e,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"glossary-of-physical-ai-terms"},"Glossary of Physical AI Terms"),(0,r.yg)("h2",{id:"a"},"A"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Artificial Intelligence (AI)"),": The simulation of human intelligence processes by computer systems, especially machine learning and deep learning algorithms."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Autonomous Vehicle"),": A vehicle capable of sensing its environment and operating without human involvement, typically using perception-action systems."),(0,r.yg)("h2",{id:"c"},"C"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Camera Calibration"),": The process of determining the intrinsic and extrinsic parameters of a camera to correct for distortions and establish accurate measurements."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Closed-loop Control"),": A control system that uses feedback to continuously adjust its behavior based on the difference between desired and actual output."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Computer Vision"),": A field of artificial intelligence that trains computers to interpret and understand the visual world, using digital images/video and machine learning."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Control Loop Frequency"),": The rate at which a control system updates its actions based on feedback, typically measured in Hertz (Hz)."),(0,r.yg)("h2",{id:"d"},"D"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Deep Learning"),": A subset of machine learning that uses artificial neural networks with multiple layers to model and understand complex patterns."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Depth Perception"),": The ability to perceive the world in three dimensions and judge the distance of objects, critical in stereo vision systems."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Direct Method (SLAM)"),": A category of visual SLAM approaches that use pixel intensities directly rather than extracting features."),(0,r.yg)("h2",{id:"e"},"E"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Embodied Cognition"),": The theory that cognitive processes are influenced by the body's interactions with the environment."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Embodiment"),": The concept that intelligence emerges from the interaction between an agent and its physical environment."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Encoder"),": A device that measures the rotation of a motor or wheel, providing relative position information for robot navigation."),(0,r.yg)("h2",{id:"f"},"F"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Feature Detection"),": The technique of identifying distinctive points in an image that can be used for matching, tracking, or recognition."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Feature Matching"),": The process of finding corresponding features between different images or frames."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Field of View (FOV)"),": The extent of the observable world that a camera can see at any given moment."),(0,r.yg)("h2",{id:"h"},"H"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Hybrid Approach (Visual Servoing)"),": A method that combines both position-based and image-based visual servoing techniques."),(0,r.yg)("h2",{id:"i"},"I"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Image-Based Visual Servoing (IBVS)"),": A visual servoing approach that directly uses image features to compute control actions without explicitly estimating 3D pose."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Image Transport"),": A ROS package that provides transparent support for transporting images in various compressed formats."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Inertial Measurement Unit (IMU)"),": A device that measures and reports a body's specific force, angular rate, and sometimes magnetic field, used for navigation and motion tracking."),(0,r.yg)("h2",{id:"k"},"K"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Kalman Filter"),": A mathematical method that uses a series of measurements observed over time to estimate unknown variables, often used in sensor fusion."),(0,r.yg)("h2",{id:"l"},"L"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Latency"),": The time delay between a stimulus and the resulting response, critical in real-time perception-action systems."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Loop Closure"),": In SLAM, the process of recognizing when a robot returns to a previously visited location to correct accumulated drift."),(0,r.yg)("h2",{id:"m"},"M"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Machine Learning"),": A type of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Multi-sensor Fusion"),": The process of combining data from multiple sensors to improve perception accuracy and robustness."),(0,r.yg)("h2",{id:"o"},"O"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Object Detection"),": The computer vision task of identifying and locating objects within an image or video."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"OpenCV"),": An open-source computer vision and machine learning software library."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Optical Flow"),": The pattern of apparent motion of objects, surfaces, and edges in a visual scene caused by the relative motion between an observer and the scene."),(0,r.yg)("h2",{id:"p"},"P"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Perception-Action Loop"),": The continuous cycle of sensing the environment, processing information, taking action, and receiving feedback."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Position-Based Visual Servoing (PBVS)"),": A visual servoing approach that estimates the 3D pose of the target object and computes the required camera motion."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Point Cloud"),": A set of data points in space, representing the external surface of an object or environment, typically generated by 3D scanners or depth cameras."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Pose Estimation"),": The process of determining the position and orientation of an object or camera in 3D space."),(0,r.yg)("h2",{id:"r"},"R"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Real-time Processing"),": Processing data as it becomes available, with timing constraints that require immediate response."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"RGB-D Camera"),": A camera that captures both color (RGB) and depth (D) information simultaneously."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Robotic Operating System (ROS)"),": Flexible framework for writing robot software, providing services like hardware abstraction, device drivers, and message passing."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"ROS 2"),": The second generation of the Robot Operating System, designed for production environments with improved security and real-time capabilities."),(0,r.yg)("h2",{id:"s"},"S"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Sensor Fusion"),": The process of combining data from multiple sensors to achieve improved accuracy and reliability."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Simultaneous Localization and Mapping (SLAM)"),": The computational problem of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location within it."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Stereo Vision"),": A technique that extracts depth information from 2D images using two or more cameras positioned at different angles."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Structure from Motion (SfM)"),": A photogrammetric range imaging technique for estimating 3D structures from 2D image sequences."),(0,r.yg)("h2",{id:"v"},"V"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Visual Servoing"),": A control strategy that uses visual feedback to control the motion of a robot."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Vision Pipeline"),": The sequence of processing steps applied to visual data, from acquisition to action planning."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Vision Processing"),": The application of algorithms to extract meaningful information from visual data."),(0,r.yg)("h2",{id:"w"},"W"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Wheel Encoder"),": A sensor that measures the rotation of a robot's wheels to estimate distance traveled and assist in localization."))}g.isMDXComponent=!0},5680(e,t,n){n.d(t,{xA:()=>c,yg:()=>m});var a=n(6540);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter(function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable})),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach(function(t){r(e,t,n[t])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach(function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))})}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},g={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef(function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=p(n),m=r,d=u["".concat(l,".").concat(m)]||u[m]||g[m]||o;return n?a.createElement(d,i(i({ref:t},c),{},{components:n})):a.createElement(d,i({ref:t},c))});function m(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var p=2;p<o;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"}}]);